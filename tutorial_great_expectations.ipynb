{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great Expectations tutorial\n",
    "Welcome! In this tutorial we'll have a look at Great Expectations, a framework that aids you in keeping an eye on your data quality. It provides a batteries-included solution for testing and documenting your data, so that nobody has to run into any surprises when consuming it. To achieve this, you create _expectation suites_. \n",
    "\n",
    "**You can think of them as unit tests, but for data.** \n",
    "\n",
    "They also double as documentation for your dataset, so that you won't have to repeat yourself.\n",
    "\n",
    "What do we mean by data quality? Well, bad quality data can happen for different reasons. Usually, data has bad quality if its **structure** (for example the columns and their types in a table) or its **contents** (specific cells in a table) are not what you expected.\n",
    "\n",
    "For more background on Great Expectations and the problems it solves, we can recommend the authors' blogpost: [Down with Pipeline debt / Introducing Great Expectations](https://medium.com/@expectgreatdata/down-with-pipeline-debt-introducing-great-expectations-862ddc46782a). It's a good read!\n",
    "\n",
    "## What is Great Expectations (GX) exactly?\n",
    "\n",
    "<img src='figures/in_out.png' width=800px>\n",
    "\n",
    "When working with GX you use the following five core components to access, store, and manage underlying objects and processes:\n",
    "- **Data Context:**  Manages the settings and metadata for a GX project, and provides an entry point to the GX Python API.\n",
    "- **Data Sources:**  Connects to your Data Source, and organizes retrieved data for future use.\n",
    "- **Expectations:**  Identifies the standards to which your data should conform.\n",
    "- **Checkpoints:** Validates a set of Expectations against a specific set of data.\n",
    "- **Data Docs:**  Creates a web-based documentation site for your data.\n",
    "\n",
    "\n",
    "## In this tutorial\n",
    "We'll give you a brief introduction to the main concepts used in Great Expectations, walking you through writing your first expectations and generating your first data report. We have added many references to the official documentation that you can reference to when you are configuring your own setup.\n",
    "\n",
    "Contents:\n",
    "- [Data Context](#section-data-context)\n",
    "- [Data Sources](#section-data-sources)\n",
    "- [The Expectation Suite](#section-expectation-suite)\n",
    "- [Checkpoints](#section-checkpoints)\n",
    "- [Data Docs](#section-data-docs)\n",
    "- [Actions](#section-actions)\n",
    "- [Data Assistant](#section-data-assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running on Google Colab\n",
    "\n",
    "If you are running this on Google Colab, make sure to run the cell below to set everything up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "if [[ ! -d gx ]]\n",
    "then \n",
    "  git init\n",
    "  git remote add origin https://github.com/Robin069/tutorial-great-expectations.git\n",
    "  git pull origin main\n",
    "  pip install -r requirements.txt\n",
    "  apt-get install tree\n",
    "  mkdir data\n",
    "  python -c \"import pandas as pd; pd.read_csv('https://raw.githubusercontent.com/great-expectations/gx_tutorials/main/data/yellow_tripdata_sample_2019-01.csv').to_csv('data/yellow_tripdata_sample_2019-01.csv', index=False); pd.read_csv('https://raw.githubusercontent.com/great-expectations/gx_tutorials/main/data/yellow_tripdata_sample_2019-02.csv').to_csv('data/yellow_tripdata_sample_2019-02.csv', index=False)\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's jump into it then!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as gx\n",
    "import os\n",
    "import shutil\n",
    "if os.path.exists(\"gx\"):\n",
    "    shutil.rmtree(\"gx\")\n",
    "print(gx.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-data-context\"></a>\n",
    "## Data Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = gx.get_context(project_root_dir=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's take a moment to look at the `DataContext`, which represents your Great Expectations setup. It consists of a directory holding configuration files, named `gx` by default.\n",
    "\n",
    "Note: we are omitting the `uncommitted` directory here. It contains output files (such as rendered data docs), which are not part of the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree gx -nI 'uncommitted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main configuration is located in `great_expectations.yml`. We won't go into all the details here, you can refer to the [data context reference](https://docs.greatexpectations.io/docs/conceptual_guides/gx_overview#data-context) for that. \n",
    "\n",
    "Instead, we'll just introduce some concepts you'll want to be familiar with:\n",
    "\n",
    "- A **data source** provides a standard API for accessing and interacting with data from a wide variety of source systems. \n",
    "\n",
    "    Great Expectations ships with a number of built-in data sources, including:\n",
    "\n",
    "    - Pandas\n",
    "    - SQL\n",
    "    - Spark\n",
    "    - CSV\n",
    "    - Excel\n",
    "    - BigQuery\n",
    "    - Snowflake\n",
    "    - Redshift\n",
    "    - Postgres\n",
    "    - MySQL\n",
    "    - ...\n",
    "\n",
    "    You can also create your own custom data sources.\n",
    "\n",
    "\n",
    "    **No matter which Data Source you use, the Data Source's API remains the same.**\n",
    "\n",
    "- A **data asset** is one dataset that lives in a *data source*, such as an SQL table.\n",
    "- **stores** can be used to configure how expectation and validation data will be stored. See [configuring metadata stores](https://docs.greatexpectations.io/en/latest/guides/how_to_guides/configuring_metadata_stores.html) if you're interested.\n",
    "\n",
    "These are all configured in the `great_expectations.yml` file. We'll have a brief look at its contents now, but don't mind it too much, this is here for illustration purposes only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/data_context_flowchart.png\" width=1200px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat gx/great_expectations.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-data-sources\"></a>\n",
    "## Data Sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/datasource_flowchart.png\" width=1200px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load our dataset, `yellow_tripdata_sample_2019-01.csv` and create a Validator object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = context.sources.pandas_default.read_csv(\n",
    "    \"data/yellow_tripdata_sample_2019-01.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Validator` is the primary object used to configure and run validation of data assets. Validators store information about the data asset they are validating, including the expectations that have been set, and the results of those validations. \n",
    "\n",
    "One validation run can include multiple batches and expectation suites. This way, it is possible to test multiple files in the same run. Compare this to how one run of your test suite can test multiple software modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the documentation that came with the data:\n",
    " - **vendor_id** - A code indicating the TPEP provider that provided the record. 1= Creative Mobile Technologies, LLC; 2= VeriFone Inc.\n",
    " - **pickup_datetime** - The date and time when the meter was engaged.\n",
    " - **dropoff_datetime** - The date and time when the meter was disengaged.\n",
    " - **passenger_count** - The number of passengers in the vehicle. This is a driver-entered value.\n",
    " - **trip_distance** - The elapsed trip distance in miles reported by the taximeter.\n",
    " - **rate_code_id** - The final rate code in effect at the end of the trip. 1= Standard rate, 2=JFK, 3=Newark, 4=Nassau or Westchester, 5=Negotiated fare, 6=Group ride\n",
    " - **store_and_fwd_flag** - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor, aka “store and forward,” because the vehicle did not have a connection to the server. Y= store and forward trip, N= not a store and forward trip\n",
    " - **pickup_location_id** - TLC Taxi Zone in which the taximeter was engaged\n",
    " - **dropoff_location_id** - TLC Taxi Zone in which the taximeter was disengaged\n",
    " - **payment_type** - A numeric code signifying how the passenger paid for the trip. 1= Credit card, 2= Cash, 3= No charge, 4= Dispute, 5= Unknown, 6= Voided trip\n",
    " - **fare_amount** - The time-and-distance fare calculated by the meter.\n",
    " - **extra** - Miscellaneous extras and surcharges. Currently, this only includes the \\$0.50 and \\$1 rush hour and overnight charges.\n",
    " - **mta_tax** - \\$0.50 MTA tax that is automatically triggered based on the metered rate in use.\n",
    " - **tip_amount** - This field is automatically populated for credit card tips. Cash tips are not included.\n",
    " - **tolls_amount** - Total amount of all tolls paid in trip.\n",
    " - **improvement_surcharge** - \\$0.30 improvement surcharge assessed trips at the flag drop. The improvement surcharge began being levied in 2015.\n",
    " - **total_amount** - The total amount charged to passengers. Does not include cash tips.\n",
    " - **congestion_surcharge** - $2.50 surcharge for all trips that begin, end or pass through the Manhattan exclusionary zone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-expectation-suite\"></a>\n",
    "## The Expectation Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These descriptions sure help us to understand the dataset a bit better, but they don't exactly provide much guarantees. When consuming this dataset, what expectations can we have? Will the `passenger_count` field always be specified? Will the `dropoff_datetime` field always be in the same format? Is the total amount always positive?\n",
    "\n",
    "Great Expectations helps us to codify these properties in a set of `Expectations`. An `Expectation` is something that you expect to be true in your data. Again, think of it as an unit test for your dataset.\n",
    "\n",
    "**An Expectation is a verifiable assertion about data.** Expectations enhance communication about your data and improve quality for data applications. They help you take the implicit assumptions about your data and make them explicit.\n",
    "\n",
    "There are many built-in Expectations, see https://greatexpectations.io/expectations/ for a full list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_values_to_not_be_null(\"pickup_datetime\")\n",
    "validator.expect_column_values_to_be_between(\"passenger_count\", auto=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save our Expectations to a file so we can reuse them later. Expectations are stored in the *expectation store*, which by default is the `expectations` folder inside your configuration, but you can use other storage backends as well, such as a SQL database or cloud storage (S3, Azure Blob Storage or GCS). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.save_expectation_suite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-checkpoints\"></a>\n",
    "## Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the Expectation Suite against our data. For this, we'll use a `Checkpoint`. A Checkpoint is a collection of Expectations and a Data Asset. It is a way to package up a test suite and apply it to a data asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = context.add_or_update_checkpoint(\n",
    "    name=\"checkpoint_1\",\n",
    "    validator=validator,\n",
    ")\n",
    "checkpoint_result = checkpoint.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkpoint runs are the primary way to use Great Expectations in automated workflows. They are designed to be used in automated data quality pipelines, and can be run from the command line, from notebooks, or from any other Python code. They produce a JSON-formatted validation result document that can be used for further analysis or as a report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/checkpoint_flowchart.png\" width=1200px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we covered the basics, let's get to some fancier expectations. For example, we could make sure that all date columns are in the expected format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_values_to_match_strftime_format('pickup_datetime', \"%Y-%m-%d %H:%M:%S\")\n",
    "validator.expect_column_values_to_match_strftime_format('dropoff_datetime', \"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.save_expectation_suite()\n",
    "checkpoint = context.add_or_update_checkpoint(\n",
    "    name=\"checkpoint_2\",\n",
    "    validator=validator,\n",
    ")\n",
    "\n",
    "checkpoint_result = checkpoint.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the expectations we've implemented so far have been met by our data. But what happens if we run into a problem? Let's try it out by adding a new expectation that checks if the `vendor_id` is always 1 or 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.expect_column_values_to_be_in_set('vendor_id', [1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That failed. Great Expectations helpfully collected the values which do not match the expectation for us. By default, it will collect up to 20 examples of values that didn't meet the expectation (that's why it's called the _partial_ unexpected list). \n",
    "\n",
    "In the Data Docs, you can see that the expectation failed, and you can also see the unexpected values. This is very useful when you are trying to debug your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "checkpoint = context.add_or_update_checkpoint(\n",
    "    name=\"checkpoint_3\",\n",
    "    validator=validator,\n",
    ")\n",
    "\n",
    "checkpoint_result = checkpoint.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-data-docs\"></a>\n",
    "## Data Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can render these results to a friendly report, called a `Data Doc`. These data docs will describe the expectations that the data should meet, as well as the metrics detailing how well the data meets the requirements. This is how Great Expectations combines testing with documenting. Data Docs are essentially static HTML pages that can be hosted on any web server or cloud storage provider.\n",
    "\n",
    "We already built the Data Docs using the `Validator` object in the previous section. Let's check them out now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.open_data_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-actions\"></a>\n",
    "## Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most powerful features of Checkpoints is that you can configure them to run Actions. The Validation Results generated when a Checkpoint runs determine what Actions are performed. Typical use cases include sending email, Slack messages, or custom notifications. \n",
    "\n",
    "Actions can be used to do anything you are capable of programming in Python. Actions are a versatile tool for integrating Checkpoints in your pipeline's workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/actions.png\" width=800px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-data-assistant\"></a>\n",
    "## Data Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous sections we explored how we could get some metrics about our data using expectations by defining them ourselves. This requires some knowledge about the data, and some effort to write the expectations.\n",
    "\n",
    "But what if you don't know what exactly to expect of your data? This is where the `Data Assistant` comes in.\n",
    "\n",
    "A Data Assistant is a pre-configured utility that simplifies the creation of Expectations. A Data Assistant can help you determine a starting point when working with a large, new, or complex dataset by asking questions and then building a list of relevant Metrics from the answers to those questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"gx\"):\n",
    "    shutil.rmtree(\"gx\")\n",
    "try:\n",
    "    del context\n",
    "except NameError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = gx.get_context(project_root_dir=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.sources.add_pandas_filesystem(\n",
    "    \"taxi_multi_batch_datasource\",\n",
    "    base_directory=\"./data\",  # replace with your data directory\n",
    ").add_csv_asset(\n",
    "    \"all_years\",\n",
    "    batching_regex=r\"yellow_tripdata_sample_(?P<year>\\d{4})-(?P<month>\\d{2})\\.csv\",\n",
    ")\n",
    "\n",
    "all_years_asset = context.datasources[\n",
    "    \"taxi_multi_batch_datasource\"\n",
    "].get_asset(\"all_years\")\n",
    "\n",
    "multi_batch_all_years_batch_request = (\n",
    "    all_years_asset.build_batch_request()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectation_suite_name = \"my_onboarding_assistant_suite\"\n",
    "\n",
    "expectation_suite = context.add_or_update_expectation_suite(\n",
    "    expectation_suite_name=expectation_suite_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_assistant_result = context.assistants.onboarding.run(\n",
    "    batch_request=multi_batch_all_years_batch_request,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectation_suite = data_assistant_result.get_expectation_suite(\n",
    "    expectation_suite_name=expectation_suite_name\n",
    ")\n",
    "\n",
    "context.add_or_update_expectation_suite(expectation_suite=expectation_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = context.add_or_update_checkpoint(\n",
    "    name=f\"yellow_tripdata_sample_{expectation_suite_name}\",\n",
    "    validations=[\n",
    "        {\n",
    "            \"batch_request\": multi_batch_all_years_batch_request,\n",
    "            \"expectation_suite_name\": expectation_suite_name,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "checkpoint_result = checkpoint.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.open_data_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to view Batch-level visualizations of the Metrics computed by the Onboarding Data Assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_assistant_result.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_assistant_result.plot_expectations_and_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very powerful tool, and it can help you to get started with your data. However, it is not a silver bullet. It is still up to you to decide which metrics are important, and which are not. For example, the Data Assistant will not tell you that the `vendor_id` field should only contain the values 1 and 2. It will only tell you that the field is categorical, and that it contains 2 unique values. It is up to you to decide whether this is what you expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our tutorial. We hope you enjoyed it, and that you are now ready to start using Great Expectations in your own projects! \n",
    "\n",
    "**Any feedback is welcome!** If you have any questions, remarks, or suggestions, please let us know! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
